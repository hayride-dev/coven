package hayride:ai@0.0.62;

interface runner {
    use types.{message};
    use agents.{agent};
    use model.{format};
    use wasi:io/streams@0.2.0.{output-stream};
    use tensor-stream.{tensor-stream};
    use inference-stream.{graph-execution-context-stream};

    enum error-code {
        invoke-error,
        unknown
    }

    resource error {
        /// return the error code.
        code: func() -> error-code;
        /// errors can propagated with backend specific status through a string value.
        data: func() -> string;
    }

    resource %stream {
        constructor(tensor-stream: tensor-stream);
        chunk: func(format: option<borrow<format>>) -> result<message,error>;
    }

    invoke: func(message: message, agent: borrow<agent>, format: borrow<format>, graph: borrow<graph-execution-context-stream>) -> result<stream, error>;
}
