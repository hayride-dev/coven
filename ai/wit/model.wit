package hayride:ai@0.0.50;

interface model {
    use inference-stream.{graph-execution-context-stream};
    use types.{message};

    enum error-code {
        context-error,
        context-encode,
        context-decode,
        compute-error,
        unknown
    }

    resource error {
        /// return the error code.
        code: func() -> error-code;
        /// errors can propagated with backend specific status through a string value.
        data: func() -> string;
    }

    resource format {
        constructor(model: string);
        encode: func(messages: list<message>) -> result<list<u8>,error>;
        decode: func(raw: list<u8>) -> result<message, error>;
        model: func() -> string;
    }

    resource model {
        constructor(format: format, graph: graph-execution-context-stream);

        // todo: add options to compute so we can set model specific options
        compute: func(messages: list<message>) -> result<message,error>;    
    }
}